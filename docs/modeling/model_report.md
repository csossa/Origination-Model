# Final Model Report
_Report describing the final model to be delivered - typically comprised of one or more of the models built during the life of the project_

## Analytic Approach
What is target definition Es la variable dependiente binaria que puede tomar dos posibles valores, que se etiquetar√° con 0 y 1.

What are inputs (description) Es el conjunto de n variables independientes (ùë•1, ùë•2, ‚Ä¶ , ùë•ùëõ) relacionadas con la informaci√≥n propia del solicitante, tomadas con el fin de explicar y/o predecir el valor de Y ( Dias_mora)

What kind of model was built? Se construy√≥ un modelo xgboost con el conjunto de datos de la entidad financiera para definir la precisi√≥n del modelo de scoring de originaci√≥n.

## Solution Description

Se construy√≥ un modelo xgboots, el cual posee tres elementos principales: Una funci√≥n de p√©rdida a optimizar, un algoritmo de aprendizaje d√©bil para realizar las predicciones y un modelo aditivo para a√±adir los algoritmos de aprendizaje d√©biles que minimizan la funci√≥n de p√©rdida.

## Data
* Source
* Data Schema
* Sampling
* Selection (dates, segments)
* Stats (counts)

## Features
* List of raw and derived features 

![image](https://user-images.githubusercontent.com/111644646/207732596-c7a12be2-c89b-46e0-87eb-df00cb73288c.png)

* Importance ranking.
La Figura muestra la importancia de predictores de acuerdo con el modelo XGBoost, donde se observa que ‚Äúmonto‚Äù es tambi√©n el predictor que m√°s pesa para determinar si se otorga una obligaci√≥n a una solicitud de cr√©dito.

![image](https://user-images.githubusercontent.com/111644646/207732118-ffcd54fb-154a-4a1e-9b44-2d6eb5c59571.png)


## Algorithm
* Description or images of data flow graph
  * if AzureML, link to:
    * Training experiment
    * Scoring workflow
* What learner(s) were used?
* Learner hyper-parameters

XGBoost es uno algoritmo predictivo de machine learning de tipo supervisado  que se caracteriza por obtener buenos resultados de predicci√≥n con  relativamente poco esfuerzo, en muchos casos mejores que los devueltos por modelos m√°s complejos computacionalmente, en particular para problemas con datos heterog√©neos

Durante el entrenamiento, los par√°metros de cada modelo d√©bil son ajustados iterativamente tratando de encontrar el m√≠nimo de una funci√≥n objetivo, que puede ser la proporci√≥n de error en la clasificaci√≥n, el √°rea bajo la curva (AUC), la ra√≠z del error cuadr√°tico medio (RMSE) o alguna otra.

## Results
Se entrena un modelo XGBoost que logra un accuracy de 90 %

![image](https://user-images.githubusercontent.com/111644646/207730439-d7fb8f28-a231-49bc-8fb3-2cfd23e5d483.png)

El modelo XGBoost fue m√°s preciso que los modelos de regresi√≥n log√≠stica, Random Forest y perceptron multicapa.
